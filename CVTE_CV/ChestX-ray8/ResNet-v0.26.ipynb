{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dataset: Chest X-Ray8\n",
    "https://www.kaggle.com/nih-chest-xrays/data\n",
    "https://nihcc.app.box.com/v/ChestXray-NIHCC/folder/36938765345\n",
    "1) 112,120 X-ray images with disease labels from 30,805 unique patients\n",
    "2）Label:['Atelectasis', 'Cardiomegaly', 'Effusion','Infiltration', 'Mass', 'Nodule', 'Pneumonia', \\\n",
    "        'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image, ImageDraw\n",
    "import scipy.ndimage.filters as filters\n",
    "from scipy.ndimage import binary_dilation\n",
    "import scipy.ndimage as ndimage\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os,gc\n",
    "import sys\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import itertools  \n",
    "from sklearn.metrics import confusion_matrix,roc_curve,accuracy_score,auc,roc_auc_score \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "torch.cuda.set_device(3)\n",
    "print (torch.cuda.current_device())\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1,2,3,4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78468 / 78468 The length of trainset is 78468\n",
      "11219 / 11219 The length of validset is 11219\n",
      "22433 / 22433 The length of testset is 22433\n",
      "The length of boxset is 984\n"
     ]
    }
   ],
   "source": [
    "def Image_Processing(img_path, crop_size=224):\n",
    "    img = Image.open(img_path).convert('RGB').resize((256, 256),Image.ANTIALIAS) #open and resize\n",
    "    #crop and normalize\n",
    "    transform_sequence = transforms.Compose([\n",
    "                                             #transforms.ToPILImage(), #if not PILImage\n",
    "                                             transforms.CenterCrop(crop_size),\n",
    "                                             #transforms.RandomCrop(crop_size),\n",
    "                                             #transforms.RandomHorizontalFlip(),\n",
    "                                             transforms.ToTensor(), # range [0, 255] -> [0.0,1.0]\n",
    "                                             transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "                                            ])\n",
    "    img = transform_sequence(img).numpy() #tensor to numpy\n",
    "    return img\n",
    "img_path = '/data/fjsdata/NIH-CXR/images/images/' \n",
    "CLASS_NAMES = ['Atelectasis', 'Cardiomegaly', 'Effusion','Infiltration', 'Mass', 'Nodule', 'Pneumonia','Pneumothorax', \\\n",
    "               'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia'] \n",
    "#preparing the trainset and  testset\n",
    "trN, trI, trY = [],[],[]\n",
    "with open('/data/fjsdata/NIH-CXR/chexnet_dataset/train.txt', \"r\") as file_descriptor: #tarinset\n",
    "    lines = file_descriptor.readlines()\n",
    "    for line in lines:\n",
    "        try:\n",
    "            line_items = line.split()\n",
    "            image_name = line_items[0].split('/')[1]\n",
    "            trN.append(image_name)\n",
    "            image_label = line_items[1:]  # 14 labels from index 2\n",
    "            image_label = [int(i) for i in image_label]  \n",
    "            trY.append(np.array(image_label))\n",
    "            img = Image_Processing(os.path.join(img_path, image_name))\n",
    "            trI.append(img)\n",
    "        except:\n",
    "            print(image_name+\":\"+str(os.path.join(img_path, image_name)))\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trN),78468))\n",
    "        sys.stdout.flush()\n",
    "trI = np.array(trI)\n",
    "trY = np.array(trY)   \n",
    "print('The length of trainset is %d'%len(trN))\n",
    "        \n",
    "valN, valI, valY = [],[],[]\n",
    "with open('/data/fjsdata/NIH-CXR/chexnet_dataset/val.txt', \"r\") as file_descriptor: #valset\n",
    "    lines = file_descriptor.readlines()\n",
    "    for line in lines:\n",
    "        try:\n",
    "            line_items = line.split()\n",
    "            image_name = line_items[0].split('/')[1]\n",
    "            valN.append(image_name)\n",
    "            image_label = line_items[1:]  # 14 labels from index 2\n",
    "            image_label = [int(i) for i in image_label]  \n",
    "            valY.append(np.array(image_label))\n",
    "            img = Image_Processing(os.path.join(img_path, image_name))\n",
    "            valI.append(img)\n",
    "        except:\n",
    "            print(image_name+\":\"+str(os.path.join(img_path, image_name)))\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(valN),11219))\n",
    "        sys.stdout.flush()\n",
    "valI = np.array(valI)\n",
    "valY = np.array(valY) \n",
    "print('The length of validset is %d'%len(valN))\n",
    "\n",
    "teN, teI, teY = [],[],[]\n",
    "with open('/data/fjsdata/NIH-CXR/chexnet_dataset/test.txt', \"r\") as file_descriptor: #testset\n",
    "    lines = file_descriptor.readlines()\n",
    "    for line in lines:\n",
    "        try:\n",
    "            line_items = line.split()\n",
    "            image_name = line_items[0].split('/')[1]\n",
    "            teN.append(image_name)\n",
    "            image_label = line_items[1:]  # 14 labels from index 2\n",
    "            image_label = [int(i) for i in image_label]  \n",
    "            teY.append(np.array(image_label))\n",
    "            img = Image_Processing(os.path.join(img_path, image_name))                    \n",
    "            teI.append(img)\n",
    "        except:\n",
    "            print(image_name+\":\"+str(os.path.join(img_path, image_name)))\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(teN),22433))\n",
    "        sys.stdout.flush()\n",
    "teI = np.array(teI)\n",
    "teY = np.array(teY)    \n",
    "print('The length of testset is %d'%len(teN))\n",
    "\n",
    "#preparing bounding box dataset\n",
    "boxdata = pd.read_csv(\"/data/fjsdata/NIH-CXR/chexnet_dataset/fjs_BBox.csv\" , sep=',')\n",
    "boxdata = boxdata[['Image Index','Finding Label','Bbox [x', 'y', 'w', 'h]']]\n",
    "#print('Dataset statistic, records: %d, fields: %d' % (boxdata.shape[0], boxdata.shape[1]))\n",
    "#print(boxdata.columns.values.tolist())\n",
    "bbN, bbI, bbY, bBox = [],[],[],[]\n",
    "for _, row in boxdata.iterrows():\n",
    "    bbN.append(row['Image Index'])\n",
    "    \n",
    "    img = Image_Processing(os.path.join(img_path, row['Image Index']))\n",
    "    bbI.append(img)\n",
    "    \n",
    "    labels = np.zeros(len(CLASS_NAMES))\n",
    "    labels[CLASS_NAMES.index(row['Finding Label'])] = 1\n",
    "    bbY.append(labels)\n",
    "    \n",
    "    bBox.append(np.array([row['Bbox [x'], row['y'], row['w'], row['h]']])) #xywh  \n",
    "print('The length of boxset is %d'%len(bbN))\n",
    "bbI = np.array(bbI)\n",
    "bbY = np.array(bbY)\n",
    "bBox = np.array(bBox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return torch.sigmoid(x) \n",
    "\n",
    "\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(t_num_classes=14, pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
    "        num_fc_ftr = model.fc.in_features #overwrite the fc layer\n",
    "        model.fc = torch.nn.Linear(num_fc_ftr, t_num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
    "    return model\n",
    "\n",
    "def compute_AUCs(gt, pred):\n",
    "    AUROCs = []\n",
    "    gt_np = gt.cpu().numpy()\n",
    "    pred_np = pred.cpu().numpy()\n",
    "    for i in range(N_CLASSES):\n",
    "        AUROCs.append(roc_auc_score(gt_np[:, i], pred_np[:, i]))\n",
    "    return AUROCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 351 / 351 : validation loss = 0.157047Eopch:     1 val_loss = 0.170023 avg_auroc= 0.694372\n",
      " 351 / 351 : validation loss = 0.170212Eopch:     2 val_loss = 0.169117 avg_auroc= 0.714963\n",
      " 351 / 351 : validation loss = 0.151512Eopch:     3 val_loss = 0.168893 avg_auroc= 0.731633\n",
      " 351 / 351 : validation loss = 0.155991Eopch:     4 val_loss = 0.167883 avg_auroc= 0.744922\n",
      " 351 / 351 : validation loss = 0.157113Eopch:     5 val_loss = 0.180345 avg_auroc= 0.724041\n",
      " 351 / 351 : validation loss = 0.159564Eopch:     6 val_loss = 0.166156 avg_auroc= 0.762309\n",
      " 351 / 351 : validation loss = 0.135223Eopch:     7 val_loss = 0.161969 avg_auroc= 0.748798\n",
      " 351 / 351 : validation loss = 0.145159Eopch:     8 val_loss = 0.161730 avg_auroc= 0.763553\n",
      " 351 / 351 : validation loss = 0.140552Eopch:     9 val_loss = 0.160097 avg_auroc= 0.777045\n",
      " 351 / 351 : validation loss = 0.129725Eopch:    10 val_loss = 0.159004 avg_auroc= 0.771089\n",
      " 351 / 351 : validation loss = 0.134646Eopch:    11 val_loss = 0.157000 avg_auroc= 0.774422\n",
      " 351 / 351 : validation loss = 0.145958Eopch:    12 val_loss = 0.158705 avg_auroc= 0.779587\n",
      " 351 / 351 : validation loss = 0.126894Eopch:    13 val_loss = 0.160553 avg_auroc= 0.771542\n",
      " 351 / 351 : validation loss = 0.149659Eopch:    14 val_loss = 0.163353 avg_auroc= 0.774829\n",
      " 351 / 351 : validation loss = 0.137259Eopch:    15 val_loss = 0.158915 avg_auroc= 0.779469\n",
      " 351 / 351 : validation loss = 0.124345Eopch:    16 val_loss = 0.157121 avg_auroc= 0.783857\n",
      " 351 / 351 : validation loss = 0.131235Eopch:    17 val_loss = 0.158060 avg_auroc= 0.783322\n",
      " 351 / 351 : validation loss = 0.125888Eopch:    18 val_loss = 0.153157 avg_auroc= 0.797286\n",
      " 351 / 351 : validation loss = 0.123545Eopch:    19 val_loss = 0.153554 avg_auroc= 0.796224\n",
      " 351 / 351 : validation loss = 0.121238Eopch:    20 val_loss = 0.153778 avg_auroc= 0.795101\n",
      " 351 / 351 : validation loss = 0.120501Eopch:    21 val_loss = 0.153757 avg_auroc= 0.796271\n",
      " 351 / 351 : validation loss = 0.120449Eopch:    22 val_loss = 0.154214 avg_auroc= 0.794189\n",
      " 351 / 351 : validation loss = 0.119896Eopch:    23 val_loss = 0.153867 avg_auroc= 0.796348\n",
      " 351 / 351 : validation loss = 0.123335Eopch:    24 val_loss = 0.154366 avg_auroc= 0.794371\n",
      " 351 / 351 : validation loss = 0.119583Eopch:    25 val_loss = 0.154174 avg_auroc= 0.794598\n",
      " 351 / 351 : validation loss = 0.120664Eopch:    26 val_loss = 0.154243 avg_auroc= 0.794218\n",
      " 351 / 351 : validation loss = 0.120228Eopch:    27 val_loss = 0.154210 avg_auroc= 0.794230\n",
      " 351 / 351 : validation loss = 0.121533Eopch:    28 val_loss = 0.154714 avg_auroc= 0.793392\n",
      " 351 / 351 : validation loss = 0.119583Eopch:    29 val_loss = 0.154720 avg_auroc= 0.793461\n",
      " 351 / 351 : validation loss = 0.118827Eopch:    30 val_loss = 0.154541 avg_auroc= 0.793786\n",
      " 351 / 351 : validation loss = 0.119508Eopch:    31 val_loss = 0.154928 avg_auroc= 0.793260\n",
      " 351 / 351 : validation loss = 0.120788Eopch:    32 val_loss = 0.154632 avg_auroc= 0.793427\n",
      " 351 / 351 : validation loss = 0.118973Eopch:    33 val_loss = 0.155279 avg_auroc= 0.793557\n",
      " 351 / 351 : validation loss = 0.120535Eopch:    34 val_loss = 0.154961 avg_auroc= 0.793374\n",
      " 351 / 351 : validation loss = 0.121052Eopch:    35 val_loss = 0.154559 avg_auroc= 0.793637\n",
      " 351 / 351 : validation loss = 0.121314Eopch:    36 val_loss = 0.154675 avg_auroc= 0.793362\n",
      " 351 / 351 : validation loss = 0.117874Eopch:    37 val_loss = 0.154555 avg_auroc= 0.794064\n",
      " 351 / 351 : validation loss = 0.118192Eopch:    38 val_loss = 0.154665 avg_auroc= 0.793783\n",
      " 351 / 351 : validation loss = 0.118949Eopch:    39 val_loss = 0.154852 avg_auroc= 0.793469\n",
      " 351 / 351 : validation loss = 0.117382Eopch:    40 val_loss = 0.154500 avg_auroc= 0.794058\n",
      " 351 / 351 : validation loss = 0.117393Eopch:    41 val_loss = 0.154667 avg_auroc= 0.794063\n",
      " 351 / 351 : validation loss = 0.118184Eopch:    42 val_loss = 0.154639 avg_auroc= 0.794075\n",
      " 351 / 351 : validation loss = 0.119427Eopch:    43 val_loss = 0.154758 avg_auroc= 0.793420\n",
      " 351 / 351 : validation loss = 0.119051Eopch:    44 val_loss = 0.154631 avg_auroc= 0.793781\n",
      " 351 / 351 : validation loss = 0.120156Eopch:    45 val_loss = 0.154833 avg_auroc= 0.794114\n",
      " 351 / 351 : validation loss = 0.118638Eopch:    46 val_loss = 0.154663 avg_auroc= 0.794029\n",
      " 351 / 351 : validation loss = 0.119971Eopch:    47 val_loss = 0.154462 avg_auroc= 0.793544\n",
      " 351 / 351 : validation loss = 0.119978Eopch:    48 val_loss = 0.154737 avg_auroc= 0.793785\n",
      " 351 / 351 : validation loss = 0.119157Eopch:    49 val_loss = 0.154665 avg_auroc= 0.793458\n",
      " 351 / 351 : validation loss = 0.119163Eopch:    50 val_loss = 0.154547 avg_auroc= 0.793227\n",
      "best_loss = 0.153157 best_auroc = 0.797286\n"
     ]
    }
   ],
   "source": [
    "#model training\n",
    "N_CLASSES = len(CLASS_NAMES) #class numbers\n",
    "model = resnet50(t_num_classes=N_CLASSES, pretrained=True).cuda()#initialize model\n",
    "#model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3, 4, 5, 6, 7]).cuda()# make model available multi GPU cores training\n",
    "torch.backends.cudnn.benchmark = True  # improve train speed slightly\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5, mode='min')\n",
    "criterion = torch.nn.BCELoss()\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "AUROC_best = 0.\n",
    "batchSize = 32 #'Batch Size': 32\n",
    "for epoch in range(50):#'Max Epoch': 50\n",
    "    model.train()  # set network as train mode\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trY)))\n",
    "    num_batches = len(shuffled_idx) // batchSize + 1\n",
    "    with torch.autograd.enable_grad():\n",
    "        for i in range(num_batches):\n",
    "            optimizer.zero_grad()#grad vanish\n",
    "            min_idx = i * batchSize\n",
    "            max_idx = np.min([len(shuffled_idx), (i+1)*batchSize])\n",
    "            selected_idx = shuffled_idx[min_idx:max_idx]\n",
    "            I_batch = torch.from_numpy(trI[selected_idx]).type(torch.FloatTensor).cuda()\n",
    "            y_batch = torch.from_numpy(trY[selected_idx]).type(torch.FloatTensor).cuda()\n",
    "            #forword\n",
    "            y_outputs = model(I_batch)#permute the dims of matrix， .permute(0, 3, 1, 2)\n",
    "            #loss\n",
    "            loss = criterion(y_outputs, y_batch)\n",
    "            loss.backward()\n",
    "            #update parameters\n",
    "            optimizer.step()\n",
    "            sys.stdout.write('\\r {} / {} : train loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "            sys.stdout.flush()     \n",
    "    #validation process\n",
    "    gt = torch.FloatTensor().cuda()\n",
    "    pred = torch.FloatTensor().cuda()\n",
    "    loss_val = []\n",
    "    mean_loss_tensor = 0.\n",
    "    num_batches = len(valY) // batchSize  +1\n",
    "    model.eval()  # set network as eval mode without BN & Dropout\n",
    "    with torch.autograd.no_grad():\n",
    "        for j in range(num_batches):\n",
    "            min_idx = j * batchSize\n",
    "            max_idx = np.min([len(valY), (j+1)*batchSize])\n",
    "            I_batch = torch.from_numpy(valI[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "            y_batch = torch.from_numpy(valY[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "            y_outputs = model(I_batch)#forword， .permute(0, 3, 1, 2)\n",
    "            curr_loss = criterion(y_outputs, y_batch)\n",
    "            gt = torch.cat((gt, y_batch), 0)\n",
    "            pred = torch.cat((pred, y_outputs.data), 0)\n",
    "            sys.stdout.write('\\r {} / {} : validation loss = {}'.format(j + 1, num_batches, float('%0.6f'%curr_loss.item()) ) )\n",
    "            sys.stdout.flush()  \n",
    "            mean_loss_tensor += curr_loss  # tensor op.\n",
    "            loss_val.append(curr_loss.item())\n",
    "    mean_loss_tensor = mean_loss_tensor / len(valY)  # tensor\n",
    "    scheduler.step(mean_loss_tensor.item())\n",
    "    AUROCs = compute_AUCs(gt, pred)\n",
    "    AUROC_avg = np.array(AUROCs).mean()\n",
    "    print(\"Eopch: %5d val_loss = %.6f avg_auroc= %.6f\" % (epoch + 1, np.mean(loss_val), AUROC_avg))\n",
    "    #if np.mean(loss_val) < best_loss:\n",
    "    if AUROC_avg > AUROC_best:\n",
    "        best_loss = np.mean(loss_val)\n",
    "        AUROC_best = AUROC_avg\n",
    "        best_net = copy.deepcopy(model)        \n",
    "print(\"best_loss = %.6f best_auroc = %0.6f\" % (best_loss, AUROC_best))\n",
    "model = model.cpu()#release gpu memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 701 / 702 The average AUROC is 0.7934\n",
      "The AUROC of Atelectasis is 0.7837\n",
      "The AUROC of Cardiomegaly is 0.8937\n",
      "The AUROC of Effusion is 0.8704\n",
      "The AUROC of Infiltration is 0.6826\n",
      "The AUROC of Mass is 0.7875\n",
      "The AUROC of Nodule is 0.7125\n",
      "The AUROC of Pneumonia is 0.7110\n",
      "The AUROC of Pneumothorax is 0.8232\n",
      "The AUROC of Consolidation is 0.7895\n",
      "The AUROC of Edema is 0.8673\n",
      "The AUROC of Emphysema is 0.8398\n",
      "The AUROC of Fibrosis is 0.7656\n",
      "The AUROC of Pleural_Thickening is 0.7398\n",
      "The AUROC of Hernia is 0.8406\n"
     ]
    }
   ],
   "source": [
    "#performance of testset\n",
    "# initialize the ground truth and output tensor\n",
    "gt = torch.FloatTensor().cuda()\n",
    "pred = torch.FloatTensor().cuda()\n",
    "num_batches = len(teY) // batchSize  +1\n",
    "best_net.eval()  # set network as eval mode without BN & Dropout\n",
    "with torch.autograd.no_grad():\n",
    "    for i in range(num_batches):\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(teY), (i+1)*batchSize])\n",
    "        I_batch = torch.from_numpy(teI[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        y_batch = torch.from_numpy(teY[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        gt = torch.cat((gt, y_batch), 0)\n",
    "        y_outputs = best_net(I_batch)#forword，.permute(0, 3, 1, 2)\n",
    "        pred = torch.cat((pred, y_outputs.data), 0)\n",
    "        sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "CLASS_NAMES = ['Atelectasis', 'Cardiomegaly', 'Effusion','Infiltration', 'Mass', 'Nodule', 'Pneumonia', \\\n",
    "               'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia'] \n",
    "def compute_AUCs(gt, pred):\n",
    "    AUROCs = []\n",
    "    gt_np = gt.cpu().numpy()\n",
    "    pred_np = pred.cpu().numpy()\n",
    "    for i in range(N_CLASSES):\n",
    "        AUROCs.append(roc_auc_score(gt_np[:, i], pred_np[:, i]))\n",
    "    return AUROCs\n",
    "\n",
    "AUROCs = compute_AUCs(gt, pred)\n",
    "AUROC_avg = np.array(AUROCs).mean()\n",
    "print('The average AUROC is {AUROC_avg:.4f}'.format(AUROC_avg=AUROC_avg))\n",
    "for i in range(N_CLASSES):\n",
    "    print('The AUROC of {} is {:.4f}'.format(CLASS_NAMES[i], AUROCs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30 / 31 The average AUROC is 0.7461\n",
      "The AUROC of Atelectasis is 0.6842\n",
      "The AUROC of Cardiomegaly is 0.9241\n",
      "The AUROC of Effusion is 0.7946\n",
      "The AUROC of Infiltration is 0.6729\n",
      "The AUROC of Mass is 0.7560\n",
      "The AUROC of Nodule is 0.6724\n",
      "The AUROC of Pneumonia is 0.6026\n",
      "The AUROC of Pneumothorax is 0.8621\n"
     ]
    }
   ],
   "source": [
    "#performance of box\n",
    "# initialize the ground truth and output tensor\n",
    "gt = torch.FloatTensor().cuda()\n",
    "pred = torch.FloatTensor().cuda()\n",
    "num_batches = len(bbY) // batchSize  +1\n",
    "best_net.eval()  # set network as eval mode without BN & Dropout\n",
    "with torch.autograd.no_grad():\n",
    "    for i in range(num_batches):\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(bbY), (i+1)*batchSize])\n",
    "        I_batch = torch.from_numpy(np.array(bbI)[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        y_batch = torch.from_numpy(np.array(bbY)[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        gt = torch.cat((gt, y_batch), 0)\n",
    "        y_outputs = best_net(I_batch)#forword，.permute(0, 3, 1, 2)\n",
    "        pred = torch.cat((pred, y_outputs.data), 0)\n",
    "        sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "def compute_AUCs(gt, pred):\n",
    "    AUROCs = []\n",
    "    gt_np = gt.cpu().numpy()\n",
    "    pred_np = pred.cpu().numpy()\n",
    "    for i in [0, 1, 2, 3, 4, 5, 6, 7]:\n",
    "        AUROCs.append(roc_auc_score(gt_np[:, i], pred_np[:, i]))\n",
    "    return AUROCs\n",
    "\n",
    "AUROCs = compute_AUCs(gt, pred)\n",
    "AUROC_avg = np.array(AUROCs).mean()\n",
    "print('The average AUROC is {AUROC_avg:.4f}'.format(AUROC_avg=AUROC_avg))\n",
    "for i in [0, 1, 2, 3, 4, 5, 6, 7]:\n",
    "    print('The AUROC of {} is {:.4f}'.format(CLASS_NAMES[i], AUROCs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
